{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare audio data for image recognition\n",
    "\n",
    "The data is pretty good, but there's a few samples that aren't exactly 1 second long and some samples that are either truncated or don't contain very much of the word.\n",
    "\n",
    "The code in the notebook attempts to filter out the broken audio so that we are only using good audio.\n",
    "\n",
    "We then generate spectrograms of each word. We mix in background noise with the words to make it a more realistic audio sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download data set\n",
    "Download from: https://storage.cloud.google.com/download.tensorflow.org/data/speech_commands_v0.02.tar.gz - approx 2.3 GB\n",
    "\n",
    "And then run\n",
    "```\n",
    "tar -xzf data_speech_commands_v0.02.tar.gz -C speech_data\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow_io'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m gfile\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow_io\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtfio\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m gen_audio_ops \u001b[38;5;28;01mas\u001b[39;00m audio_ops\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnotebook\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow_io'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.io import gfile\n",
    "import tensorflow_io as tfio\n",
    "from tensorflow.python.ops import gen_audio_ops as audio_ops\n",
    "from tqdm.notebook import tqdm as tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speech_data\n"
     ]
    }
   ],
   "source": [
    "SPEECH_DATA='speech_data'\n",
    "print(SPEECH_DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The audio is all sampled at 16KHz and should all be 1 second in length - so 1 second is 16000 samples\n",
    "EXPECTED_SAMPLES=16000\n",
    "# Noise floor to detect if any audio is present\n",
    "NOISE_FLOOR=0.1\n",
    "# How many samples should be abover the noise floor?\n",
    "MINIMUM_VOICE_LENGTH=EXPECTED_SAMPLES/4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of folders we want to process in the speech_data folder\n",
    "from tensorflow.python.ops import gen_audio_ops as audio_ops\n",
    "words = [\n",
    "    'backward',\n",
    "    'bed',\n",
    "    'bird',\n",
    "    'cat',\n",
    "    'dog',\n",
    "    'down',\n",
    "    'eight',\n",
    "    'five',\n",
    "    'follow',\n",
    "    'forward',\n",
    "    'four',\n",
    "    'go',\n",
    "    'happy',\n",
    "    'house',\n",
    "    'learn',\n",
    "    'left',\n",
    "    'marvin',\n",
    "    'nine',\n",
    "    'no',\n",
    "    'off',\n",
    "    'on',\n",
    "    'one',\n",
    "    'right',\n",
    "    'seven',\n",
    "    'sheila',\n",
    "    'six',\n",
    "    'stop',\n",
    "    'three',\n",
    "    'tree',\n",
    "    'two',\n",
    "    'up',\n",
    "    'visual',\n",
    "    'wow',\n",
    "    'yes',\n",
    "    'zero',\n",
    "    '_background',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all the files in a directory\n",
    "def get_files(word):\n",
    "    return gfile.glob(\"../\"+SPEECH_DATA + '/'+word+'/'+'*'+'.wav')\n",
    "\n",
    "# get the location of the voice\n",
    "def get_voice_position(audio, noise_floor):\n",
    "    audio = audio - np.mean(audio)\n",
    "    audio = audio / np.max(np.abs(audio))\n",
    "    return tfio.audio.trim(audio, axis=0, epsilon=noise_floor)\n",
    "\n",
    "# Work out how much of the audio file is actually voice\n",
    "def get_voice_length(audio, noise_floor):\n",
    "    position = get_voice_position(audio, noise_floor)\n",
    "    return (position[1] - position[0]).numpy()\n",
    "\n",
    "# is enough voice present?\n",
    "def is_voice_present(audio, noise_floor, required_length):\n",
    "    voice_length = get_voice_length(audio, noise_floor)\n",
    "    return voice_length >= required_length\n",
    "\n",
    "# is the audio the correct length?\n",
    "def is_correct_length(audio, expected_length):\n",
    "    return (audio.shape[0]==expected_length).numpy()\n",
    "\n",
    "\n",
    "def is_valid_file(file_name):\n",
    "    # load the audio file\n",
    "    audio_tensor = tfio.audio.AudioIOTensor(file_name)\n",
    "    # check the file is long enough\n",
    "    if not is_correct_length(audio_tensor, EXPECTED_SAMPLES):\n",
    "        return False\n",
    "    # convert the audio to an array of floats and scale it to betweem -1 and 1\n",
    "    audio = tf.cast(audio_tensor[:], tf.float32)\n",
    "    audio = audio - np.mean(audio)\n",
    "    audio = audio / np.max(np.abs(audio))\n",
    "    # is there any voice in the audio?\n",
    "    if not is_voice_present(audio, NOISE_FLOOR, MINIMUM_VOICE_LENGTH):\n",
    "        return False\n",
    "    return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spectrogram(audio):\n",
    "    # normalise the audio\n",
    "    audio = audio - np.mean(audio)\n",
    "    audio = audio / np.max(np.abs(audio))\n",
    "    # create the spectrogram\n",
    "    spectrogram = audio_ops.audio_spectrogram(audio,\n",
    "                                              window_size=320,\n",
    "                                              stride=160,\n",
    "                                              magnitude_squared=True).numpy()\n",
    "    # reduce the number of frequency bins in our spectrogram to a more sensible level\n",
    "    spectrogram = tf.nn.pool(\n",
    "        input=tf.expand_dims(spectrogram, -1),\n",
    "        window_shape=[1, 6],\n",
    "        strides=[1, 6],\n",
    "        pooling_type='AVG',\n",
    "        padding='SAME')\n",
    "    spectrogram = tf.squeeze(spectrogram, axis=0)\n",
    "    spectrogram = np.log10(spectrogram + 1e-6)\n",
    "    return spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process a file into its spectrogram\n",
    "def process_file(file_path):\n",
    "    # load the audio file\n",
    "    audio_tensor = tfio.audio.AudioIOTensor(file_path)\n",
    "    # convert the audio to an array of floats and scale it to betweem -1 and 1\n",
    "    audio = tf.cast(audio_tensor[:], tf.float32)\n",
    "    audio = audio - np.mean(audio)\n",
    "    audio = audio / np.max(np.abs(audio))\n",
    "    # randomly reposition the audio in the sample\n",
    "    voice_start, voice_end = get_voice_position(audio, NOISE_FLOOR)\n",
    "    end_gap=len(audio) - voice_end\n",
    "    random_offset = np.random.uniform(0, voice_start+end_gap)\n",
    "    audio = np.roll(audio,-random_offset+end_gap)\n",
    "    # add some random background noise\n",
    "    background_volume = np.random.uniform(0, 0.1)\n",
    "    # get the background noise files\n",
    "    background_files = get_files('_background_noise_')\n",
    "    background_file = np.random.choice(background_files)\n",
    "    background_tensor = tfio.audio.AudioIOTensor(background_file)\n",
    "    background_start = np.random.randint(0, len(background_tensor) - 16000)\n",
    "    # normalise the background noise\n",
    "    background = tf.cast(background_tensor[background_start:background_start+16000], tf.float32)\n",
    "    background = background - np.mean(background)\n",
    "    background = background / np.max(np.abs(background))\n",
    "    # mix the audio with the scaled background\n",
    "    audio = audio + background_volume * background\n",
    "    # get the spectrogram\n",
    "    return get_spectrogram(audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = []\n",
    "validate = []\n",
    "test = []\n",
    "\n",
    "TRAIN_SIZE=0.8\n",
    "VALIDATION_SIZE=0.1\n",
    "TEST_SIZE=0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "351300aff37242f389b6d11a385229fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing words:   0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "backward\n",
      "0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b469081fe5f447bbbf7237729c59d32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Checking:   0%|          | 0/1664 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow.io' has no attribute 'audio'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 50\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m word:\n\u001b[1;32m     48\u001b[0m         \u001b[38;5;66;03m# add more examples of marvin to balance our training set\u001b[39;00m\n\u001b[1;32m     49\u001b[0m         repeat \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m70\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m word \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmarvin\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 50\u001b[0m         \u001b[43mprocess_word\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrepeat\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepeat\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(train), \u001b[38;5;28mlen\u001b[39m(test), \u001b[38;5;28mlen\u001b[39m(validate))\n",
      "Cell \u001b[0;32mIn[21], line 12\u001b[0m, in \u001b[0;36mprocess_word\u001b[0;34m(word, repeat)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(label)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# get a list of files names for the word\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m file_names \u001b[38;5;241m=\u001b[39m [file_name \u001b[38;5;28;01mfor\u001b[39;00m file_name \u001b[38;5;129;01min\u001b[39;00m tqdm(get_files(word), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChecking\u001b[39m\u001b[38;5;124m\"\u001b[39m, leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;28;01mif\u001b[39;00m is_valid_file(file_name)]\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(file_names)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# randomly shuffle the filenames\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[21], line 12\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(label)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# get a list of files names for the word\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m file_names \u001b[38;5;241m=\u001b[39m [file_name \u001b[38;5;28;01mfor\u001b[39;00m file_name \u001b[38;5;129;01min\u001b[39;00m tqdm(get_files(word), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChecking\u001b[39m\u001b[38;5;124m\"\u001b[39m, leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mis_valid_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_name\u001b[49m\u001b[43m)\u001b[49m]\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(file_names)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# randomly shuffle the filenames\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[17], line 28\u001b[0m, in \u001b[0;36mis_valid_file\u001b[0;34m(file_name)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mis_valid_file\u001b[39m(file_name):\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;66;03m# load the audio file\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m     audio_tensor \u001b[38;5;241m=\u001b[39m \u001b[43mtfio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maudio\u001b[49m\u001b[38;5;241m.\u001b[39mAudioIOTensor(file_name)\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;66;03m# check the file is long enough\u001b[39;00m\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_correct_length(audio_tensor, EXPECTED_SAMPLES):\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow.io' has no attribute 'audio'"
     ]
    }
   ],
   "source": [
    "def process_files(file_names, label, repeat=1):\n",
    "    file_names = tf.repeat(file_names, repeat).numpy()\n",
    "    return [(process_file(file_name), label) for file_name in tqdm(file_names, desc=f\"{word} ({label})\", leave=False)]\n",
    "\n",
    "# process the files for a word into the spectrogram and one hot encoding word value\n",
    "def process_word(word, repeat=1):\n",
    "    # the index of the word word we are processing\n",
    "    label = words.index(word)\n",
    "    print(word)\n",
    "    print(label)\n",
    "    # get a list of files names for the word\n",
    "    file_names = [file_name for file_name in tqdm(get_files(word), desc=\"Checking\", leave=False) if is_valid_file(file_name)]\n",
    "    print(file_names)\n",
    "    # randomly shuffle the filenames\n",
    "    np.random.shuffle(file_names)\n",
    "    # split the files into train, validate and test buckets\n",
    "    train_size=int(TRAIN_SIZE*len(file_names))\n",
    "    validation_size=int(VALIDATION_SIZE*len(file_names))\n",
    "    test_size=int(TEST_SIZE*len(file_names))\n",
    "    # get the training samples\n",
    "    train.extend(\n",
    "        process_files(\n",
    "            file_names[:train_size],\n",
    "            label,\n",
    "            repeat=repeat\n",
    "        )\n",
    "    )\n",
    "    # and the validation samples\n",
    "    validate.extend(\n",
    "        process_files(\n",
    "            file_names[train_size:train_size+validation_size],\n",
    "            label,\n",
    "            repeat=repeat\n",
    "        )\n",
    "    )\n",
    "    # and the test samples\n",
    "    test.extend(\n",
    "        process_files(\n",
    "            file_names[train_size+validation_size:],\n",
    "            label,\n",
    "            repeat=repeat\n",
    "        )\n",
    "    )\n",
    "\n",
    "# process all the words and all the files\n",
    "for word in tqdm(words, desc=\"Processing words\"):\n",
    "    if '_' not in word:\n",
    "        # add more examples of marvin to balance our training set\n",
    "        repeat = 70 if word == 'marvin' else 1\n",
    "        process_word(word, repeat=repeat)\n",
    "    \n",
    "print(len(train), len(test), len(validate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5ed08d6e0c54e919b4ee87158d85ff5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Background Noise:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'tfio' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 47\u001b[0m\n\u001b[1;32m     43\u001b[0m     test\u001b[38;5;241m.\u001b[39mextend(samples[train_size\u001b[38;5;241m+\u001b[39mvalidation_size:])\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file_name \u001b[38;5;129;01min\u001b[39;00m tqdm(get_files(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_background_noise_\u001b[39m\u001b[38;5;124m'\u001b[39m), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing Background Noise\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m---> 47\u001b[0m     \u001b[43mprocess_background\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwords\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m_background\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(train), \u001b[38;5;28mlen\u001b[39m(test), \u001b[38;5;28mlen\u001b[39m(validate))\n",
      "Cell \u001b[0;32mIn[13], line 4\u001b[0m, in \u001b[0;36mprocess_background\u001b[0;34m(file_name, label)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess_background\u001b[39m(file_name, label):\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;66;03m# load the audio file\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m     audio_tensor \u001b[38;5;241m=\u001b[39m \u001b[43mtfio\u001b[49m\u001b[38;5;241m.\u001b[39maudio\u001b[38;5;241m.\u001b[39mAudioIOTensor(file_name)\n\u001b[1;32m      5\u001b[0m     audio \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mcast(audio_tensor[:], tf\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m      6\u001b[0m     audio_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(audio)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tfio' is not defined"
     ]
    }
   ],
   "source": [
    "# process the background noise files\n",
    "def process_background(file_name, label):\n",
    "    # load the audio file\n",
    "    audio_tensor = tfio.audio.AudioIOTensor(file_name)\n",
    "    audio = tf.cast(audio_tensor[:], tf.float32)\n",
    "    audio_length = len(audio)\n",
    "    samples = []\n",
    "    for section_start in tqdm(range(0, audio_length-EXPECTED_SAMPLES, 8000), desc=file_name, leave=False):\n",
    "        section_end = section_start + EXPECTED_SAMPLES\n",
    "        section = audio[section_start:section_end]\n",
    "        # get the spectrogram\n",
    "        spectrogram = get_spectrogram(section)\n",
    "        samples.append((spectrogram, label))\n",
    "\n",
    "    # simulate random utterances\n",
    "    for section_index in tqdm(range(1000), desc=\"Simulated Words\", leave=False):\n",
    "        section_start = np.random.randint(0, audio_length - EXPECTED_SAMPLES)\n",
    "        section_end = section_start + EXPECTED_SAMPLES\n",
    "        section = np.reshape(audio[section_start:section_end], (EXPECTED_SAMPLES))\n",
    "\n",
    "        result = np.zeros((EXPECTED_SAMPLES))\n",
    "        # create a pseudo bit of voice\n",
    "        voice_length = np.random.randint(MINIMUM_VOICE_LENGTH/2, EXPECTED_SAMPLES)\n",
    "        voice_start = np.random.randint(0, EXPECTED_SAMPLES - voice_length)\n",
    "        hamming = np.hamming(voice_length)\n",
    "        # amplify the voice section\n",
    "        result[voice_start:voice_start+voice_length] = hamming * section[voice_start:voice_start+voice_length]\n",
    "        # get the spectrogram\n",
    "        spectrogram = get_spectrogram(np.reshape(section, (16000, 1)))\n",
    "        samples.append((spectrogram, label))\n",
    "        \n",
    "    \n",
    "    np.random.shuffle(samples)\n",
    "    \n",
    "    train_size=int(TRAIN_SIZE*len(samples))\n",
    "    validation_size=int(VALIDATION_SIZE*len(samples))\n",
    "    test_size=int(TEST_SIZE*len(samples))\n",
    "    \n",
    "    train.extend(samples[:train_size])\n",
    "\n",
    "    validate.extend(samples[train_size:train_size+validation_size])\n",
    "\n",
    "    test.extend(samples[train_size+validation_size:])\n",
    "\n",
    "        \n",
    "for file_name in tqdm(get_files('_background_noise_'), desc=\"Processing Background Noise\"):\n",
    "    process_background(file_name, words.index(\"_background\"))\n",
    "    \n",
    "print(len(train), len(test), len(validate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56c74bdfab6f4bfaa6a72c9c4e4743a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing problem noise: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def process_problem_noise(file_name, label):\n",
    "    samples = []\n",
    "    # load the audio file\n",
    "    audio_tensor = tfio.audio.AudioIOTensor(file_name)\n",
    "    audio = tf.cast(audio_tensor[:], tf.float32)\n",
    "    audio_length = len(audio)\n",
    "    samples = []\n",
    "    for section_start in tqdm(range(0, audio_length-EXPECTED_SAMPLES, 400), desc=file_name, leave=False):\n",
    "        section_end = section_start + EXPECTED_SAMPLES\n",
    "        section = audio[section_start:section_end]\n",
    "        # get the spectrogram\n",
    "        spectrogram = get_spectrogram(section)\n",
    "        samples.append((spectrogram, label))\n",
    "        \n",
    "    np.random.shuffle(samples)\n",
    "    \n",
    "    train_size=int(TRAIN_SIZE*len(samples))\n",
    "    validation_size=int(VALIDATION_SIZE*len(samples))\n",
    "    test_size=int(TEST_SIZE*len(samples))\n",
    "    \n",
    "    train.extend(samples[:train_size])\n",
    "    validate.extend(samples[train_size:train_size+validation_size])\n",
    "    test.extend(samples[train_size+validation_size:])\n",
    "\n",
    "\n",
    "for file_name in tqdm(get_files(\"_problem_noise_\"), desc=\"Processing problem noise\"):\n",
    "    process_problem_noise(file_name, words.index(\"_background\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6ae4766b63c4d27bca3c61bb8445879",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing problem noise: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def process_mar_sounds(file_name, label):\n",
    "    samples = []\n",
    "    # load the audio file\n",
    "    audio_tensor = tfio.audio.AudioIOTensor(file_name)\n",
    "    audio = tf.cast(audio_tensor[:], tf.float32)\n",
    "    audio_length = len(audio)\n",
    "    samples = []\n",
    "    for section_start in tqdm(range(0, audio_length-EXPECTED_SAMPLES, 4000), desc=file_name, leave=False):\n",
    "        section_end = section_start + EXPECTED_SAMPLES\n",
    "        section = audio[section_start:section_end]\n",
    "        section = section - np.mean(section)\n",
    "        section = section / np.max(np.abs(section))\n",
    "        # add some random background noise\n",
    "        background_volume = np.random.uniform(0, 0.1)\n",
    "        # get the background noise files\n",
    "        background_files = get_files('_background_noise_')\n",
    "        background_file = np.random.choice(background_files)\n",
    "        background_tensor = tfio.audio.AudioIOTensor(background_file)\n",
    "        background_start = np.random.randint(0, len(background_tensor) - 16000)\n",
    "        # normalise the background noise\n",
    "        background = tf.cast(background_tensor[background_start:background_start+16000], tf.float32)\n",
    "        background = background - np.mean(background)\n",
    "        background = background / np.max(np.abs(background))\n",
    "        # mix the audio with the scaled background\n",
    "        section = section + background_volume * background\n",
    "        # get the spectrogram\n",
    "        spectrogram = get_spectrogram(section)\n",
    "        samples.append((spectrogram, label))\n",
    "        \n",
    "    np.random.shuffle(samples)\n",
    "    \n",
    "    train_size=int(TRAIN_SIZE*len(samples))\n",
    "    validation_size=int(VALIDATION_SIZE*len(samples))\n",
    "    test_size=int(TEST_SIZE*len(samples))\n",
    "    \n",
    "    train.extend(samples[:train_size])\n",
    "    validate.extend(samples[train_size:train_size+validation_size])\n",
    "    test.extend(samples[train_size+validation_size:])\n",
    "\n",
    "\n",
    "for file_name in tqdm(get_files(\"_mar_sounds_\"), desc=\"Processing problem noise\"):\n",
    "    process_mar_sounds(file_name, words.index(\"_background\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 0\n"
     ]
    }
   ],
   "source": [
    "print(len(train), len(test), len(validate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomise the training samples\n",
    "np.random.shuffle(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/z3/03wq9pbn33vfv89mzfw6vjfc0000gp/T/ipykernel_31367/2607822258.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mX_validate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_validate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 0)"
     ]
    }
   ],
   "source": [
    "X_train, Y_train = zip(*train)\n",
    "X_validate, Y_validate = zip(*validate)\n",
    "X_test, Y_test = zip(*test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the computed data\n",
    "np.savez_compressed(\n",
    "    \"training_spectrogram.npz\",\n",
    "    X=X_train, Y=Y_train)\n",
    "print(\"Saved training data\")\n",
    "np.savez_compressed(\n",
    "    \"validation_spectrogram.npz\",\n",
    "    X=X_validate, Y=Y_validate)\n",
    "print(\"Saved validation data\")\n",
    "np.savez_compressed(\n",
    "    \"test_spectrogram.npz\",\n",
    "    X=X_test, Y=Y_test)\n",
    "print(\"Saved test data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the width and height of the spectrogram \"image\"\n",
    "IMG_WIDTH=X_train[0].shape[0]\n",
    "IMG_HEIGHT=X_train[0].shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_images2(images_arr, imageWidth, imageHeight):\n",
    "    fig, axes = plt.subplots(5, 5, figsize=(10, 20))\n",
    "    axes = axes.flatten()\n",
    "    for img, ax in zip(images_arr, axes):\n",
    "        ax.imshow(np.reshape(img, (imageWidth, imageHeight)))\n",
    "        ax.axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = words.index(\"marvin\")\n",
    "\n",
    "X_marvins = np.array(X_train)[np.array(Y_train) == word_index]\n",
    "Y_marvins = np.array(Y_train)[np.array(Y_train) == word_index]\n",
    "plot_images2(X_marvins[:20], IMG_WIDTH, IMG_HEIGHT)\n",
    "print(Y_marvins[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = words.index(\"yes\")\n",
    "\n",
    "X_yes = np.array(X_train)[np.array(Y_train) == word_index]\n",
    "Y_yes = np.array(Y_train)[np.array(Y_train) == word_index]\n",
    "plot_images2(X_yes[:20], IMG_WIDTH, IMG_HEIGHT)\n",
    "print(Y_yes[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
